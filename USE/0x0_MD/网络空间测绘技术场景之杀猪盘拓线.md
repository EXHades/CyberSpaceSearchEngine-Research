# [网络空间测绘技术场景之杀猪盘拓线](https://mp.weixin.qq.com/s/PPwFs3mxKDwkRlUQVzfXUQ)

白帽汇赵武 2021-08-13

任何尖端的技术都需要考虑落地的场景，没有场景就很难持续发展。



就比如说核技术的研究，当年科学家们很兴奋地推导了e=mc平方的公式，他们单纯地从探索宇宙的角度去深入挖掘各种可能性，发现可以通过对原子的操作来释放大量的能量。这个技术是一个新的突破，但是用在哪个场景科学家们还没有来得及去思考，或者由于他们对物质的无所谓态度导致他们根本不会去思考。如果没有足够好的应用场景，那么这种科学就不会得到足够多的关注和资源。科学家们把这些研究公开讨论了，这就跟现在的开源是一个意思，开放就是为了让全世界来帮忙找到在各行各业可以落地的应用场景，由此变成可持续发展的路线。很不幸，从我们老百姓的角度，大多数人第一次听说核技术并不是什么核能发电或者是医疗教育，而是把一个城市夷为平地的"小男孩"原子弹，它最初的场景是一种战争的武器，也是一种战略压制。好在，随着后来技术和人类文明的进一步成熟，大家把它变成了各种可控的可以造福人类的技术，总算回归到了正规。



同样的，从电子管到晶体管二极管到后来的CPU，最初他们是一种军事工具，后来他们找到了收音机和袖珍计算器的场景，他们找到了PC的场景，从加法器到电子计算跨越了整整一个世纪。一个具有人类里程碑意义的技术的发展，他们都会经历过一段时间，一个世纪的时间对于宇宙的进化来说实在是太短了，几乎可以忽略不计，但是对于人类个体的生命而言，通常就对应一辈子。



技术的理论研究和可行性证明，甚至是原型提出，通常都会比较快，但是把它变成全世界推广的基础技术，通常会非常非常困难。从这一点来说，技术又不能过于领先地去做研究，倒不是说做突破是不对的，而是历史已经多次证明：在缺乏落地场景缺乏可持续发展的商务支持的前提下，单纯的技术研究很容易成为先烈。那些勇于探索的科学家很值得敬佩，伟大如尼古拉特斯拉这样划时代的科学家，但如果能够更实用主义者一点，有团队帮他来保障落地完成资金的积累，可能他就不至于后来那么穷困潦倒，也就能给人类留下更多的比交流电更好的惠及全人类的技术。科学家们活着不是为了完成资金和资源的积累，积累是一个过程而不是结果，科学家们完成资源积累的目的，是为了没有后顾之忧地更好的去完成新的技术突破。



回到网络空间测绘技术而言，大家很容易就完成了demo的输出，无论是端口扫描和协议识别以及指纹识别等，都到了一个一说大家都明白的地步。很快大家就抛出了新的问题：网络空间测绘技术到底能做什么？通常这种越简单的问题越难回答，就好像弗莱明刚做出来电子管那会，成本高昂故障率高，完成功能特别简单，我们可以脑补一下如果当时有人问弗莱明：你做出的这个玩意除了无线接受器还能干啥？我估计如果他脾气暴躁一点就容易恼羞成怒，他很难想象未来的科学家会基于他的粗糙模型（从现在眼光来看实在是太粗糙了）做出什么样伟大的各种各样的突破。



大家用得最多的两个场景就是：一）针对特定目标进行资产的收集，包括域名、证书、关键字等；二）针对特定的漏洞做地区内影响范围的判断。实际上网络空间测绘能做的当然不仅仅是这些方面，它还可以做很多的输出，只是我们自己都还没有想到而已。



最近我们针对监管的需求，做了一个"杀猪盘拓线"的功能，这个功能特别简单，用户输入一个明确是杀猪盘的网站（比如受害者报案），我们通过特征聚类，找到更多的网站，再通过网站找出更多的特征，不断的学习迭代，从而完成从1到100万的杀猪盘网站的提取。我们可以通过一个动画的来感受一下效果：





<video src="./网络空间测绘技术场景之杀猪盘拓线.assets/0bf2nyaeyaaamyafrzywrvqva3wdjrxaataa.f10002.mp4">








在做的过程中遇到很多的问题，最主要的就是在不同的线索获取的过程中，如何又多又精准。事实上精准并不是难点，人肉眼看就行，但是100万的数据量都让人肉眼看岂不又变成了不可完成的任务了。人类科技进步的体现就是人投入的成本越来越少，但是效率反而会越来越高，输出也会越来越多，核心的区别就是工具化，懒人改变了世界这句话在某些场景下还是有道理的。所以我们的任务就变成了：如何把人为确认的过程变成可自动化。一旦细化到这种子任务就简单了，那就是如何把人容易看出来的事情教机器做到，这就是典型的机器学习的场景。把大量数据做好分类标签，然后训练出识别分类的模型，只要解决这个问题就能很好的完成自动化的需求，而且准确度也非常高。



到这个逻辑，就只剩一个最大的难点了：那就是如何生成那么大量的标注数据。如果还是按照人工肉眼来看再挨个标注，你先看了100万的样本，做好了手动的标注，再来训练，似乎问题又回到了原点，一度又进入了死胡同。所以我们就想，如果fofa可以让我只标注100个数据，就能自动生成100万标注好的训练样本集，那么所有问题就引刃而解了。杀猪盘的每个网站的标题，关键字都不太一样，所以通过关键字的方式能获取的样本非常有限。



忽略后来的思考过程，中间省略一万个字，我们最终的方案就变成了这样：

1.   输入一个fofa的原始查询语句

2.   fofa完成数据的自动聚类（不通过关键字，而是一类特征，后续找机会我再做详细介绍）

3.   排序提取前100的聚类，做手动的数据标定

4.   从fofa提取特征对应的html正文，生成大量（如100万）的样本集合，并且切分成训练样本和测试样本

5.   利用训练样本进行模型的训练，用测试样本确认准确率回收率

6.   上线生产环境进行实际的预测



每次一个小技术点的突破，都有可能引发大的产业变革，这种小的创新往往来自于跨界的结合，我最近一直在思考的问题是，既然fofa有这么多的元数据，基于这种安全大数据与深度学习的结合，能发生什么样有意思的化学反应呢？虽然我还没有摸清门路，不过我感觉这个方向是很有戏的。



我编写了一个跨平台的单机工具fofafasttext，来完成整个训练学习和提供预测api的服务，我们来完整的感受一下：

-- 从fofa获取聚类特征，并且手动进行聚类特征的标注，这里我只做二分类（rubbish,not_rubbish）

./fofafasttext -mode query -query <杀猪盘关键字> -size 100 -outfile rubbish.yaml -category rubbish,not_rubbish

-- 每个特征提取10000个样本（后来测试几百个训练就够用了），切分成训练样本和测试样本

./fofafasttext -mode genData -infile rubbish.yaml -size 10000

-- 开始训练，模型保存为model_rubbish

./fofafasttext -mode supervised -infile train.txt -outfile model_rubbish

-- 用测试数据验证一下，一般都有97%以上的准确率

./fofafasttext -mode test -model model_rubbish.bin -infile test.txt

-- 本地预测：

./fofafasttext -mode predict -model model_rubbish.bin -pageurl http://rjj.ondaart.com

-- 提供预测的api：

./fofafasttext -mode predict -model model_rubbish.bin -bind :1222 

echo "rjj.ondaart.com\nwww.baidu.com" | curl 127.0.0.1:1222/check --data-binary @- 

{"rjj.ondaart.com":"rubbish","www.baidu.com":"not_rubbish"}



我直接用了fasttext的方法来做分类的工作，除去二分类，当然也可以多标签，比如把国内所有的网页做分类。



稍微总结一下：首先我们提取了杀猪盘拓线这个非常细分的场景；其次我们提炼了两个最主要的技术创新点，就是用fofa完成了网站的自动聚类算法以及网站分类模型的快速生成技术；最后我们把所有的技术流程用一个小工具全部封装完成，开箱即用，并且支持任意分类的拓展。所以，虽然我们只是解决了杀猪盘拓线的一个具体的需求，然而我们提出的技术思路可以很快的无缝拓展到其他的应用场景中去，解决更为复杂的问题。这也验证了我一句话：我们想要解决一个问题，所以我们拥有了与之对应的所有技能。



fofa是最基础的数据源头，基于这些元数据我们可以进行数据的再加工，生成满足业务场景需要的结果数据。我一直很纠结是fofa把这些加工的过程全部做了，还是说让用户自己来完成这些加工的工作？它是一个工具，还是一个生态？我们不可能把所有的场景都枚举完，即便枚举完了，基于我理解“用户一定比我们更聪明”的逻辑，我们也很难全部做完或者做好。也许一种更好的方式是提供数据，搭建框架，我们就是乐高或者minecraft，我们提供标准化的最小模块，用户可以基于这些模块完成无穷无尽的脑洞大开的创新。



一种好的技术和产品是容易让人忘记它的存在的，你想用的时候它就在那，它的存在非常的自然不露痕迹。你不用刻意地去对每一个人沟通技术指标，对比每一个产品细节，不去喧哗，不哗众取宠，甚至是不要去打扰用户，让大家静静地感受价值就好。“剩者为王”的时代，少即是多，活到最后就能赢。